{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnlKA8RQJl1yEjKVTJCe6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/auto-paper-analysis/blob/main/notebooks/analyze_arxiv_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "KV4QkPCAAwmz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n",
        "!pip install pypdf2\n",
        "!pip install fitz\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "Jpxsv3SEs4ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VEq5clsosz35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "16d8aac8-8f62-44ee-d205-818a7251f697"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "GEMINI_API_KEY=\"...\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_pdf_from_arxiv(arxiv_id):\n",
        "  url = f'http://export.arxiv.org/pdf/{arxiv_id}'\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    return response.content\n",
        "  else:\n",
        "    raise Exception(f\"Failed to download pdf for arXiv id {arxiv_id}\")\n",
        "\n",
        "# Example usage\n",
        "arxiv_id = \"2306.00001\"\n",
        "pdf_content = download_pdf_from_arxiv(arxiv_id)\n",
        "\n",
        "# Save the pdf content to a file\n",
        "with open(f\"{arxiv_id}.pdf\", \"wb\") as f:\n",
        "  f.write(pdf_content)\n"
      ],
      "metadata": {
        "id": "g9Ima2kQtLPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a064e2ae-df92-4fd7-9487-7984ff74c952"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import PyPDF2\n",
        "\n",
        "def extract_text_and_figures(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text and figures from a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists:\n",
        "            * A list of extracted text blocks.\n",
        "            * A list of extracted figures (as bytes).\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    figures = []\n",
        "\n",
        "    # Open the PDF using PyMuPDF (fitz) for image extraction\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page_num, page in enumerate(doc):\n",
        "        text = page.get_text(\"text\")  # Extract text as plain text\n",
        "        texts.append(text)\n",
        "\n",
        "        # Process images on the page\n",
        "        image_list = page.get_images()\n",
        "        for image_index, img in enumerate(image_list):\n",
        "            xref = img[0]  # Image XREF\n",
        "            pix = fitz.Pixmap(doc, xref)  # Create Pixmap image\n",
        "\n",
        "            # Save image in desired format (here, PNG)\n",
        "            if pix.n < 5:  # Grayscale or RGB\n",
        "                img_bytes = pix.tobytes(\"png\")\n",
        "            else:  # CMYK: Convert to RGB first\n",
        "                pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                img_bytes = pix.tobytes(\"png\")\n",
        "\n",
        "            figures.append(img_bytes)\n",
        "\n",
        "    # Extract additional text using PyPDF2 (in case fitz didn't get everything)\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts, figures\n"
      ],
      "metadata": {
        "id": "wci-rQVXtvHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9ea31033-b19a-4261-8f4b-d947da8a78a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts, figures = extract_text_and_figures(f'{arxiv_id}.pdf')"
      ],
      "metadata": {
        "id": "m9FQwxPyufzQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "09b575e2-a80d-495e-a9ae-703d71aaa4f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "hUovrG-5uwbS",
        "outputId": "3fb9fdcb-de65-4c01-c68c-10d56973c4e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This paper has been accepted for publication at the\\nIEEE Conference on Artiﬁcial Intelligence Circuits and Systems (AICAS), Hangzhou, 2023.\\n©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future\\nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or\\nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\\nTinyissimoYOLO: A Quantized, Low-Memory\\nFootprint, TinyML Object Detection Network for\\nLow Power Microcontrollers\\nJulian Moosmann, Marco Giordano, Christian Vogt, Michele Magno\\nCenter for Project Based Learning - ETH Z¨urich\\njulian.moosmann, marco.giordano, christian.vogt, michele.magno@pbl.ee.ethz.ch\\nAbstract—This paper introduces a highly ﬂexible, quantized,\\nmemory-efﬁcient, and ultra-lightweight object detection network,\\ncalled TinyissimoYOLO. It aims to enable object detection on\\nmicrocontrollers in the power domain of milliwatts, with less\\nthan 0.5 MB memory available for storing convolutional neural\\nnetwork (CNN) weights. The proposed quantized network archi-\\ntecture with 422 k parameters, enables real-time object detection\\non embedded microcontrollers, and it has been evaluated to\\nexploit CNN accelerators. In particular, the proposed network has\\nbeen deployed on the MAX78000 microcontroller achieving high\\nframe-rate of up to 180 fps and an ultra-low energy consumption\\nof only 196 µJ per inference with an inference efﬁciency of more\\nthan 106 MAC/Cycle. TinyissimoYOLO can be trained for any\\nmulti-object detection. However, considering the small network\\nsize, adding object detection classes will increase the size and\\nmemory consumption of the network, thus object detection\\nwith up to 3 classes is demonstrated. Furthermore, the net-\\nwork is trained using quantization-aware training and deployed\\nwith 8-bit quantization on different microcontrollers, such as\\nSTM32H7A3, STM32L4R9, Apollo4b and on the MAX78000’s\\nCNN accelerator. Performance evaluations are presented in this\\npaper.\\nIndex Terms—YOLO, ML, computer vision, object detection,\\nCNN accelerator, microcontroller, quantization, quantization-\\naware training\\nI. INTRODUCTION\\nObject detection on edge devices such as microcontroller\\nunits (µCs) have the capability of reducing detection latency\\nand increasing the overall energy efﬁciency by running on-\\ndevice network inferences [1], [2]. In addition, the sensitive\\ntransmission of sensor data is reduced or substituted, reducing\\nprivacy issues to a minimum. Furthermore, emerging dedicated\\nhardware accelerators for machine learning (ML) models are\\nenabling edge processing and edge artiﬁcial intelligence (AI)\\nreducing the energy consumption for inference and enabling\\nreal-time on-board processing on resource-constrained micro-\\ncontrollers [3]. On such constrained devices, computational\\npower is signiﬁcantly reduced and does not allow for the\\ndeployment of classical object detection deep neural networks\\n(DNNs) such as you only look once (YOLO) [4], region-based\\nconvolutional neural networks (R-CNNs) [5] or single shot\\ndetectors (SSDs) [6] because their memory requirements are\\nsigniﬁcantly exceeding the available memory of few kilobytes\\ntypically available in such devices. However, their fundamental\\nThe authors would like to thank armasuisse Science & Technology for\\nfunding this research.\\nideas are of importance for designing new DNNs, especially\\nfor the emerging ﬁeld of edge AI.\\nIn fact, to keep the power consumption in the order of a few\\nmilliwatts, the computational power on µCs is signiﬁcantly\\nlower compared to CPUs and GPUs. Thus, object detection\\nnetworks need to be carefully designed and optimized for\\nsuch tiny devices in particular to achieve a small memory\\nfootprint and a high number of operations processed per cycle\\n[7]. To overcome the memory constraints, several different\\nmethods have been recently reported in literature, such as\\npruning [8], [9], quantization [10], [11], new frameworks\\ndeveloped for memory efﬁcient inference [12], patch-based\\ninference scheduling [13] or neural architecture searches in-\\ncluding search spaces specialized for memory-constrained\\ndevices [14], [15]. These techniques are used to reduce the\\nmemory and complexity of existing state-of-the-art networks\\nto deploy on a tiny device while keeping the original network’s\\nstructure and still achieving similar inference accuracy while\\nperformance (especially inference speed) is often neglected\\n[16]. Among other techniques, quantization is one of the\\nmost promising and popular, as it reduces both the memory\\nrequirements and increases the number of operations per\\nsecond that µCs can perform. Li et al. (2019) [17] have\\nshown that quantizing an object detection network to 4-bit,\\nthey achieve state-of-the-art prediction accuracy with a mean\\naverage precision (mAP) loss of only 2 % to 5 % compared\\nto its full precision counterpart while having 8x less memory\\noccupation.\\nTo the best of our knowledge, there is still no previous work\\nthat adopts those techniques to achieve a generalized object\\ndetection network, ready for deployment on edge devices and\\nµCs with less than 0.5 MB of weight memory, that are able\\nto achieve similar accuracy performance of larger networks.\\nThis paper proposes a quantized and highly accurate object\\ndetection convolutional neural network (CNN) based on the\\narchitecture of YOLO [4] suitable for edge processors with\\nlimited memory and computational resources. The proposed\\nnetwork is composed of quantized convolutional layers with\\n3x3 kernels and a fully connected layer at the output. It is\\ndesigned for having a low memory footprint of less than\\n0.5 MB. The proposed network is trained and evaluated on\\nthe WiderFace dataset [18]. Furthermore, to showcase multi-\\nobject detection capability, while keeping the network small,\\narXiv:2306.00001v2  [cs.CV]  12 Jul 2023\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(figures))\n",
        "print(type(figures[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W06HEk3AwzJa",
        "outputId": "9f837616-a57b-4159-8281-a8875f820ada"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "<class 'bytes'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open(io.BytesIO(figures[0]))\n",
        "display(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "O-LbjMwEw7mM",
        "outputId": "92d26138-13a7-47fd-c7dc-2b85b2de1d13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1630x538>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABl4AAAIaCAIAAADOU6cpAAA5kklEQVR4nO3df5jVdZ3w//cwIz8GBoTh9w9FERNnU4s122u/urX3hbutTuZty5XuXu693UhX0LdbrSTv9pJsQW+MtSxlk9BrlS28ECIalNXcyEjzS9CWSCLlqik/BCdSYMB2GL5/nO7p5AicM5xz3p8fj8flf83M58X7/UnPefKez6kLNfGdaxqOHKnKT66vC5d+vbMaP7l6Mx9b9f5EAAAAABRrqM1l+oTQFcLZrXMq/pN/s/251TO+fdmSyrek6s18bNX7EwEAAABQrEZp7OB/hVOnXrLzP1affWmFS9PQcWeFEKrRkqo387FV708EAAAAQLE+NbvSqHMu6dc86edrFlT8J5/2nsvGvftDq2dUPvNVb+Zjq96fCAAAAIButUtjQR0rhzoGAAAAUG01TWNBHSuHOgYAAABQVbVOY0EdK4c6BgAAAFA9EdJYUMfKoY4BAAAAVEmcNBbUsXKoYwAAAADVEC2NBXWsHOoYAAAAQMXFTGNBHSuHOgYAAABQWZHTWFDHyqGOAQAAAFRQ/DQW1LFyqGMAAAAAlZKINBbUsXKoYwAAAAAVkZQ0FtSxcqhjAAAAACcuQWksqGPlUMcAAAAATlCy0lhQx8qhjgEAAACciMSlsaCOlUMdAwAAAOi1JKaxoI6VQx0DAAAA6J2EprGgjpVDHQMAAADoheSmsaCOlUMdAwAAAChXotNYUMfKoY4BAAAAlCXpaSyoY+VQxwAAAABKl4I0FtSxcqhjAAAAACVKRxoL6lg51DEAAACAUqQmjQV1rBzqGAAAAMBxpSmNBXWsHOoYAAAAwLGlLI0Fdawc6hgAAADAMaQvjQV1rBzqGAAAAMDRpDKNBXWsHOoYAAAAwNtKaxoL1a9jfaqwNuoYAAAAQHKkOI2FKtexU//4Q7989EsV/8nqGAAAAEBCpDuNhSrXsSHjWtQxAAAAgKxKfRoL6lg51DEAAACAbllIY0EdK4c6BgAAAFCQkTQW1LFyqGMAAAAAIUtpLKhj5VDHAAAAADKVxoI6Vg51DAAAAMi5rKWxoI6VQx0DAAAA8iyDaSyoY+VQxwAAAIDcymYaC+pYOdQxAAAAIJ8ym8aCOlYOdQwAAADIoSynsaCOlUMdAwAAAPIm42ksqGPlUMcAAACAXMl+GgvqWDnUMQAAACA/cpHGgjpWDnUMAAAAyIm8pLGgjpVDHQMAAADyIEdpLKhj5VDHAAAAgMzLVxoL6lg51DEAAAAg23KXxoI6Vg51DAAAAMiwPKaxoI6VQx0DAAAAsiqnaSyoY+VQxwAAAIBMym8aC+pYOdQxAAAAIHtyncaCOlYOdQwAAADImLynsaCOlUMdAwAAALJEGgtBHSuHOgYAAABkhjT2O+pY6dQxAAAAIBuksd9Tx0qnjgEAAAAZII39AXWsdOoYAAAAkHbS2FupY6VTxwAAAIBUk8behjpWOnUMAAAASC9p7O2pY6VTxwAAAICUksaOSh0rnToGAAAApJE0dizqWOnUMQAAACB1pLHjUMdKp44BAAAA6SKNHZ86Vjp1DAAAAEgRaawk6ljp1DEAAAAgLaSxUqljpVPHAAAAgFSQxsqgjpVOHQMAAACSTxorjzpWOnUMAAAASDhprGzqWOnUMQAAACDJpLHeUMdKp44BAAAAiSWN9ZI6Vjp1DAAAAEgmaaz31LHSqWMAAABAAkljJ0QdK506BgAAACSNNHai1LHSqWMAAABAokhjFaCOlU4dAwAAAJJDGqsMdax06hgAAACQENJYxahjpVPHAAAAgCSQxipJHSudOgYAAABEJ41VmDpWOnUMAAAAiEsaqzx1rHTqGAAAABCRNFYV6ljp1DEAAAAgFmmsWtSx0qljAAAAQBTSWBWpY6VTxwAAAIDak8aqSx0rnToGAAAA1Jg0VnXqWOnUMQAAAKCWpLFaUMdKp44BAAAANSON1Yg6Vjp1DAAAAKgNaax21LHSqWMAAABADUhjNaWOlU4dAwAAAKpNGqs1dax06hgAAABQVdJYBOpY6dQxAAAAoHqksTjUsdKpYwAAAECVSGPRqGOlU8cAAACAapDGYlLHSqeOAQAAABUnjUWmjpVOHQMAAAAqSxqLTx0rnToGAAAAVJA0lgjqWOnUMQAAAKBSpLGkUMdKp44BAAAAFVFXm8s8+D8a3vP3i2pzrVR79emHftv+n0fCkYr/5L2vbB00avKki6+r+E+u3szHtveVraEufPDrnTW+LgAAAJAZzt0ky6hzLtn8r7P++K9vqsYP3/jgF6rxY6s687FV6U8EAAAA5IQ0lkTDJpwde4SypXFmAAAAIOc8awwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnJLGAAAAAMgpaQwAAACAnGqIPQCkyZprGjq7wpEj0QZo6BNal3RGu/zbib4mvZPAlQQAAKD2pDEow5udoV9DaGmdM3jsWbW/+t7tWzevXtA2oyFRTSfumvROMlcSAACA2vMLlVCGriPh9GnXbWlb8MaOrbW/+tBxZ73zsjmdXaFtRoKidtw16Z1kriQAAAC1J41BeQaOmqyOvUXcNemdZK4kAAAANSaNQdnUsZ7UMQAAANJIGoPeUMd6UscAAABIHWkMekkd60kdAwAAIF2kMeg9dawndQwAAIAUydf7wH2PtpX+xf0mvaPvpDOrN0wm7Vy1vPQvHjSlpemsluoNUxvdJaildc7gsWfV+OqFprN59YK2GQ2tSzprfPWjibsmvZPMlQQAAKDa8pXG2v+tbcDQAQOGNpbyxXs3/HvTOec3fvBvqj1Vlry4cnnjsMbG5oGlfPHu768Z+Z7/Z8zfzKz2VNWmjvWkjgEAAJAK+UpjnUfCSYMH9AlHmic1l/L17f/5TMd3vqGOla7zSOh/8oA+4cjIySNK+PLhu3/5Hzu/sVgdO0HJbDrqGAAAAMmXu2eNDTp16G8Ph/bn20v54ubTh3W9/EzHd75R7amyZPDpwzsPH9n9iz2lfPHIM4Z3PP8fO7+xuNpT1YDnjvXkuWMAAAAkXO7SWFDHqk8dU8e6qWMAAAAkWR7TWFDHqk8dU8e6qWMAAAAkVk7TWFDHqk8dU8e6qWMAAAAkU37TWFDHqk8dU8e6qWMAAAAkUK7TWFDHqk8dU8e6qWMAAAAkTd7TWFDHqk8dU8e6qWMAAAAkijQWgjpWfeqYOtZNHQMAACA5pLHfUceqTR1Tx7qpYwAAACSENPZ76li1qWPqWDd1DAAAgCSQxv6AOlZt6pg61k0dAwAAIDpp7K3UsWpTx9SxbuoYAAAAcUljb0MdqzZ1TB3rpo4BAAAQkTT29tSxalPH1LFu6hgAAACxSGNHpY5VmzqmjnVTxwAAAIhCGjsWdaza1DF1rJs6BgAAQO1JY8ehjlWbOqaOdVPHAAAAqDHv5Y5v0KlD97+0d8emVwYMazzuF/epCwc3/ziE0PjBv6n+aBkx+PThb/zna7/6/14a2DzwuF9cH8Kvf/zDEMKYv5lZ/dGqq7sEtbTOGTz2rBpfvdB0Nq9e0DajoXVJZ42vfjRx16R3krmSAAAAlMKpsZIMOnXom7852Le+rpR/hgxv/M0PHo89csoMPn34ob0d/RrqSvln2MhBOx55NPbIleHsWE/OjgEAAFAz3sWVYdQZzSV+5e5fvFbVSbJqzDtGlPiVO7buruokteTsWE/OjgEAAFAbTo1BfM6O9eTsGAAAADUgjUEiqGM9qWMAAABUW3bS2IGDb97ytZV/+dEvXP2ZO5atWb//wKHYE2WHta0NdawndQwAAICqyk4au3Hh0ruXPdK/X9/d7a9/9otLr7vl3tgTZYe1rRl1rCd1DAAAgOrJSBrrPNz12JNPnzJ2xKpFc/7ltk+GEJ74SWreRSecta0xdawndQwAAIAqyUgaa6jv8/O1X1m/bP6PN/9y4ZJvhxAun3ZB7KEywtrWnjrWkzoGAABANWQkjXWbd9eDD659MoTQMnlC7FmyxtrWkjrWkzoGAABAxWUtjS2eP+vWT/9tCGHeohUHD/029jiZYm1rTB3rSR0DAACgsjKSxnbu2XvhlZ+b/fnFo4effFXrRc1Dmw50HHruhe2x58oCaxuROtaTOgYAAEAFZeR92pgRQw8cPLRm3cb6+j4nDx7YvnffwMb+kyeOjT1XFljbuLpLUKwBTqqPdeWjir4mvZPAlQQAACAjaSyEcM8ts//n/75r9WMbQgjNQ5sWff5jAwf0iz1URljbuAaOmhxCuPj6B2IN8OjtH4l16aOJvia9k8CVBAAAyLnspLF3nX36plULX9nVfrira/zo4Q31Gfld0SSwtgAAAEAmZSeNhRDq6uomjBkee4pssrYAAABA9jj+AwAAAEBOSWMAAAAA5JQ0BgAAAEBOSWMAAAAA5JQ0BgAAAEBOSWMAAAAA5JQ0BgAAAEBOSWMAAAAA5JQ0BgAAAEBOSWMAAAAA5JQ0BgAAAEBOSWMAAAAA5FTu0ljHbw7GHiHj9v+6I/YIAAAAACXJVxo75ROffu1nO9Sx6jnvH27etelldQwAAABIhXylsb6TzlTHqqppSos6BgAAAKRFvtJYUMeqTx0DAAAA0iJ3aSyoY9WnjgEAAACpkMc0FtSx6lPHAAAAgOTLaRoL6lj1qWMAAABAwuU3jQV1rPrUMQAAACDJcp3GgjpWfeoYAAAAkFh5T2NBHas+dQwAAABIJmksBHWs+tQxAAAAIIGksd9Rx6pNHQMAAACSRhr7PXWs2tQxAAAAIFGksT+gjlWbOgYAAAAkR0PsARKnUMd+defC4eeObTx5QOxxMqhQx346b+7oqRMGDWuMPQ5kU9uMFP/rvXVJZ+wRAACAvEjxe6fqUceqTR2Dajt8JNSF0DT2rNiDlK1PXV3bzGdbF6tjAABALUhjb08dqzZ1DKrqvw6H4ePPHDburPF/fFnsWcq2+Tu3qWMAAEBt5DqNHTj45h33rfnBhi0jm4d84M/e3fr+8wcN7N/9v6pj1aaOQVWdPu3a//zul0MIqatj7/zgDUmrY2uuaejsCkeOxJ4jMRr6+L1XAAAyItdp7MaFS1c/tuFdZ5++u/31z35x6fd+tPnr82cVf0FxHYs1ZLYV17HYs0AGqWOV8mZn6NcQWlrnDE7h76hW3N7tWzevXtA2o0EdAwAgA/L7CZWdh7see/LpU8aOWLVozr/c9skQwhM/2drzy7o/s7LmA2bH/o5DNy5cOvVDn7r0mnn3PPjYG/v/4NM/uz+zMtZ4kG2nT7v216/8/JWNq2MPUrZ3fvCGptFT2mYm4q9wuo6E06ddt6VtwRs73ua/FHkzdNxZ77xsTmdXuj/tAQAACvL7orahvs/P134lhLDh6V8sf/iJEMLl0y5426/8XR3754Wb1z5X0xGzYtbcux9Ys/6Cc8/cuWfvx2/62sOPb1p555ziLyjUsW23zt20ekusISHDnB2riIGjJhfqmLNj4f/WMWfHAADIgPymsW7z7nrwZ1tfDCG0TD7q7/T1nXTm2IWLazdThnQePvzQuo2nTRj1g2Xzd+7eO/F9M7//1DM9v6xpSsvU+1fUfjzICXWsItSxYuoYAADZkN9fqOy2eP6sWz/9tyGEeYtWHDz029jjZE1DfX37xqXPPXrXEz/ZetMdy0IIV7ZeGHsoyCO/WVkR3XXMb1YGv1kJAEAm5DeN7dyz98IrPzf784tHDz/5qtaLmoc2Heg49NwL22PPlVk3LLjv/lXrQgjnTTkt9iyQU+pYRahjxdQxAADSLr9pbMyIoQcOHlqzbuMn/3HJTXcsa9+7b2Bj/8kTfRJltaz46g2Lbv5YCGHObfd3HHoz9jiQU+pYRahjxdQxAABSLb9pLIRwzy2zm4c2rX5sw33fWtc8tOneWz8xcEC/2ENlzfZd7e+4ePZV198+dtSwGdOnjWwesu/AwS3bfB4lRKOOVYQ6VkwdAwAgvXL9EvZdZ5++adXCV3a1H+7qGj96eEN9rkNhlYwb3Xyg49CKtU821NcPGzJod/vrTQMHTDljfOy5INc8lb8iPJW/mKfyAwCQUnmPQXV1dRPGDJ84bqQuVj0r75ozsnnIA2vWL/rG2pHNQ1b9842DGvvHHgryztmxinB2rJizYwAApJEeRNVdcO6ZL69fsu27i5595M4XH1980flnx54ICEEdqxB1rJg6BgBA6khj1EJdXd3E8SMnnTK6ob4+9izA76ljFaGOFVPHAABIF2kMINfUsYpQx4qpYwAApIg0BpB36lhFqGPF1DEAANJCGgNAHasMdayYOgYAQCpIYwCEoI5ViDpWTB0DACD5pDEAfkcdqwh1rJg6BgBAwkljAPyeOlYR6lgxdQwAgCSTxgD4A+pYRahjxdQxAAASSxoD4K3UsYpQx4qpYwAAJJM0BsDbUMcqQh0rpo4BAJBA0hgAb08dqwh1rJg6BgBA0khjAByVOlYR6lgxdQwAgESRxgA4FnWsItSxYuoYAADJIY0BcBzqWEWoY8XUMQAAEkIaA+D41LGKUMeKqWMAACSBNAZASdSxilDHiqljAABEJ40BUCp1rCLUsWLqGAAAcUljAJRBHasIdayYOgYAQETSGADlUccqQh0rpo4BABCLNAZA2dSxilDHiqljAABEIY0B0BvqWEWoY8XUMQAAak8aA6CX1LGKUMeKqWMAANSYNAZA76ljFaGOFVPHAACoJWkMgBOijlWEOlZMHQMAoGakMQBOlDpWEepYMXUMAIDa8HITUmbT1R/uPBKO9PbbWz53c9NZLZUcKAFOcE16J5MreSJOn3btf373yyGEwWPfEXuW8pz6xx/82epn22Y2tC7ujD3L7+tYS+ucwWPPij1OZIU6tnn1grYZDa1L4u8OAACZJI1ByhzqCv37hPGTQuPAsr93//6wZf7c7DWdE1mT3snqSp6g06ddu/lfZw199ezYg5RtxCln733l57Gn+B11rJg6BgBAtUljkDJdR8LwU8Irz/emBA0aFE6dlMGmcyJr0jtZXcmKOH/6TbFH6I1Hb/9I7BF+Tx0rpo4BAFBV0hikT99GdeytTmRNeierK0lCdNex2IMkxUn1sScAACCjpDFIJXWsJ3WMjBk4anII4eLrH4g9SFIk6mQfAACZ4RMqIa26S1DHgbK/t7vp7Nu6pQqjRXMia9I7WV1JAACAnJDGIL4DB9+85Wsr//KjX7j6M3csW7N+/4FDJX5jhutYlDXpnYSvJAAAAMcgjUF8Ny5ceveyR/r367u7/fXPfnHpdbfcW/r3ZrWOxVqT3knySgIAAHAMnjUGkXUe7nrsyadPGTti1aI5r7a/fsEVNzzxk61l/YTuEtQ7J9WFbbfMnXr/il5+fxVEX5PeSeBKAgAAcGzSGETWUN/n52u/EkLY8PQvlj/8RAjh8mkXlPtD+jaGEMLUqb2cYdOmXn5jlSRhTXonaSsJAADAsUljkBTz7nrwZ1tfDCG0TJ4Qe5aksCZAFGuuaejsCkeOxJ4jMRr6hNYlnTW+qF14iyi7AAB5II1BUiyeP+t7P3r6xoX/Om/RisunvXdA/76xJ4rPmgBRvNkZ+jWEltY5g8eeFXuW+PZu37p59YK2GQ017jJ2oVisXQCAPPAYfohs5569F175udmfXzx6+MlXtV7UPLTpQMeh517YHnuumKwJEFfXkXD6tOu2tC14Y0d5DzrMpKHjznrnZXM6u0LbjJr+lapdKBZrFwAgD6QxiGzMiKEHDh5as27jJ/9xyU13LGvfu29gY//JE8fGnismawJEN3DUZF2mW6wuYxeKqWMAUCXSGMR3zy2zm4c2rX5sw33fWtc8tOneWz8xcEC/2ENFZk2A6HSZYupYEqhjAFAN/rMK8b3r7NM3rVr4yq72w11d40cPb6jXrK0JkAjdXcYTr8L/7TK1f+KVXSgWaxcAIMO824REqKurmzBm+MRxIzWgbtYESAKnloo5O5YEzo4BQGV5wwkAcCy6TDF1LAnUMQCoIGkMAOA4dJli6lgSqGMAUCnSGKTMGR+e/us3h8eeIlnOuPj9v34t9hBA1ukyxdSxJFDHAKAipDFImTGXTx923p+rY8XG/N3sYeeqY0DV6TLF1LEkUMcA4MRJY5A+6lhP6hhQG7pMMXUsCdQxADhB0hikkjrWkzoG1IYuU0wdSwJ1DABOhDQGaaWO9aSOAbWhyxRTx5JAHQOAXpPGIMXUsZ7UMaA2dJli6lgSqGMA0DvSGKSbOtaTOgbUhi5TTB1LAnUMAHrBfzUh9cZcPj2EsOOF5Tt3xh4lMcb83exwX9jx7+usCVBV3V2mpXXO4LFnxR4nskKX2bx6QduMhtYlnTW7rl0oFmsXACC9pDHIgjGXTx90Vsv+rVt69+1jKztNMoz5u9mD3vO+Xq9J72RyJYFj02WKqWNJoI4BQFmkMciIpiktTVNaYk+RLNYEqA1dppg6lgTqGACUzrPGAABOlCdeFfPcsSTw3DEAKJE0BgBQAbpMMXUsCdQxACiFNAYAUBm6TDF1LAnUMQA4LmkMAKBidJli6lgSqGMAcGw1+g9k/75hy8Nfbvmra2tzOQCi27lqeZTrDprS0nSWj18gJs+DL+ap/EngqfwAcAw1SmOtizvbZm5TxwDy48WVyxsHhcZBtb7u7u83jnzfpWMun17rC0MRXaaYOpYE6hgAHE3tjlWrYwC50nkk9G8MfUIYObrGV+7YvXndzhAyVsf2bd0Se4RKysPJPl2mmDqWBLF2Yc01DZ1d4ciRml2QE9LQJ4inQN7U9IkD6hgkyv6OQ/MXPfjoD386ZsTQyy9+719/4E8HDxoQe6jIrEllDR4e3ngt7N5V6zo2snFP9urYz+fPDSH0H9YYe5AKaOg/cNh735+l3TkaXaaYOpYEUXbhv7rCSX2C9U+FFzasfnnjtx0tBPKm1g/jVMcgOWbNvfuBNesvOPfMnXv2fvymrz38+KaVd86JPVRk1qTi1LFK+W1XOLkpNPXtaB4Ve5QK6PjVU+tChnbnGHSZYupYEtR+FzoPh3HnXWL9U+G091wWQqhqHXOKMF2cIiQnInxOjToGSdB5+PBD6zaeNmHUD5bN37l778T3zfz+U8/EHioya1Il6lilDD8lvParEELIQB07ZcQedSyf1LEkqP0ujDrnkhCC9U+FatexNztDvwanCNNh7/atHlBITvSJctXWxZ2H927b8vCXo1wdCCE01Ne3b1z63KN3PfGTrTfdsSyEcGXrhbGHisyaVM/g4aGzK+zeVevrjmzc07F5XazPyqyG4aeEfftC+6ux56iEU0bs+fVTmdqdY+juMm/s2Bp7lvgKXaazK7TNqOnf0dqFYrXfhVHnXDLynEusfyqc9p7LJvzxh6p0e3QdCf6fmBax/nUNtRftFnd2DBLihgX3bdz8yxDCeVNOiz1LUliTanB2rFKcHUup7i4Te5CkOKk+wkXtwlvUeBecHUuRqp4dc4ozRXy4LTkRs/6qY5AEK756w8OPb5o19+45t91/1QcvauzfL/ZE8VmTozlw8M077lvzgw1bRjYP+cCfvbv1/ecPGti/9G8v1LFfPR8GDqrejG+jPuz59eNrQob6izqWUgNHTQ4hXHz9A7EHSYpHb/9I7S9qF96ixrugjqWIOkaBOkYexPmFym5+sxJi2b6r/R0Xz77q+tvHjho2Y/q0kc1D9h04uGXby7HnismaHNeNC5feveyR/v367m5//bNfXHrdLfeW+xMGDw+HDoR+9bX+Z1j/jh3Z+sU9v1kJpJTfrEyRqv5mpd9xThG/WUnmxb+znR2DKMaNbj7QcWjF2icb6uuHDRm0u/31poEDppwxPvZcMVmTY+s83PXYk0+fMnbEqkVzXm1//YIrbnjiJ718LTtmTGVHK8mOHREuemwneAqv++zYgNqewquG4Y17XvzW8pCPs2OAs2Mp4uwYBc6OkW3x01hQxyCSlXfNuWL2ggfWrA8hjGwe8s0vfWpQYxlvyzPJmhxDQ32fn6/9Sghhw9O/WP7wEyGEy6ddEHuodLtx4dLVj21419mnF07hfe9Hm78+f1ZZP2H4KWHH1jCoo0oD1tTJg8OOVculMcgJdSxF1DEK1DEyLBFpLKhjEMMF55758volL23fc7ir69RxIxrqYzwSOWGsSSnm3fXgz7a+GEJomTwh9iwpVsFTeO84s7KjRbNpU+wJgBpSx1JEHaNAHSOrIj9rrJjnjkHt1dXVTRw/ctIpozWgbtbkuBbPn3Xrp/82hDBv0YqDh34be5y0KpzCW79s/o83/3Lhkm8Hp/CA/PHcsRTx3DEKPHeMTEpQGgvqGECC7dyz98IrPzf784tHDz/5qtaLmoc2Heg49NwL22PPlXrz7nrwwbVPBqfwgFxSx1JEHaNAHSN7kpXGgjoGkFRjRgw9cPDQmnUbP/mPS266Y1n73n0DG/tPnjg29lyp5xQekHPqWIqoYxSoY2RM4tJYUMcAkuqeW2Y3D21a/diG+761rnlo0723fmLggH6xh0orp/AAuqljKaKOUaCOkSUJvYk9lR8ggd519umbVi18ZVf74a6u8aOHN9Qn8e9X0qL7FF59fZ+TBw90Cg/IOU/lTxFP5afAU/nJjOS+q3F2DCCB6urqJowZPnHcSF3sxDmFB1DM2bEUcXaMAmfHyIZEv7FRxwBSbf+B2BMkW+EU3g8fuOXxb87bsPKL7z3vzNgTAUSmjqWIOkaBOkYGJDqNBXUMILXO+4ebd72ojh2HU3gAb6GOpYg6RoE6Rtql4IW4OgaQRk1TWtQxAHpBHUsRdYwCdYxUS0EaC+oYQDqpYxV0xoen79rVGHsKgBpRx1JEHaNAHSO90pHGgjoGkE7qWKWMuXz66Pdfqo4B+aGOpYg6RoE6RkqlJo0FdQwgndSxSlHHgLxRx1JEHaNAHSONUnazti7ubJu5bcvDX275q2tjzwJAqQp17Kfz5o6eGHuUlBtz+fQQwq51a0aP7og9C0AtjDrnkhDClrYFLa1zBo89K/Y4HMtp77kshPDyxm+3zWhoXdJZ2R/eXcfcCclXqGObVy+oxp1AKdpm1i71tC7OwhanLI0FdQwgnbrrWGN97FFSTh0D8kYdSxF1jAJ1LK7Ow6FPXRj37g9V+0K/2b517Se2f+DOvdW+ULWlL40FdQwgnQp1bNutczdtij1KyqljQN5017HYg1CSk6r212DqWIqoYxF1Hg6nTr2kY9ezLR/8bJUvddkLG1av/cS/p72OpTKNBXUMIJ2aprRMvX9F7CmyoLuOhaCOAbkw6pxLdj/90MXXPxB7EEry6O0fqdJPVsdSJOF17AR/6zDhv0g46pxLXn36oS3f+T/VrmOFs6Jpr2NpTWNBHQMg3wp1bMeq5dteGhh7lkrxSQ0AHF93HYs9CCWp3inCE1R3JEz6kw/37nuf/1EK/q5XHStditNYUMcAyLcxl08fdFZL7CkqZkzsAQBIi4GjJocQnCJMi+qdIjxB2U5jQR0rWbrTWFDHAMi3pinZSWMAAFSWOlaKPrEHqIDWxZ2H927b8vCXYw8CQE3t7zh048KlUz/0qUuvmXfPg4+9sf9g7IkAACBZRp1zSf8RZ2z5zv+p9oVOe89lI87+b2s/MbTaF6q4LKSxoI4B5NKsuXf/0z2rG/v327ln78dv+trff/YrsScCAIDEUceOLSNpLKhjADnTefjwQ+s2njZh1A+WzW+7+3MhhO8/9UzsodLNKTwAgKxSx44h9c8aK+a5YwD50VBf375xaQjhh5ue/ZeV3wshXNl6Yeyh0m3W3LsfWLP+gnPPLJzCe/jxTSvvnBN7qIzbdPWHu0I4fCT2HBVy6n+fXvjgVAAggTx37GgylcaCOgaQPzcsuG/j5l+GEM6bclrsWVKs+BTezt17J75vplN4NXCoK/TvE0aMDs2jYo9ywvbvDy99a3kIIY11bOeq5bFHqKQ0bgEAtaGOva2spbGgjgHkzIqv3vDw45tmzb17zm33X/XBixr794s9USo5hRdF15Ew/JTw2q9CCKmvY4MGhVMnpbWOvfSt5X1CGJryLSjYvy/85j82TPnCwtiDAJBQ6lhPGUxjQR0DyIHtu9r//Oqbpv7RpG/efv2M6dM+/5UHdre/vmXby+efc0bs0dLNKbwa69uojsX3X11h1KjwXwfChEmxRzlxo8OuXS8+e9On1TEAjkYde4vsPIb/LTyVHyDbxo1uPtBxaMXaJ6/+zB3Xzrtnd/vrTQMHTDljfOy5Um/FV29YdPPHQghzbru/49CbscdJkwMH37zlayv/8qNfuPozdyxbs37/gUMlfmOhjrW/GtpfreqAtdBdx1L3K4pNw0PDgPDy87HnqITRo0NDx4vP3vTp2IMAkFyeyl8ss2ksqGMAWbfyrjkjm4c8sGb9om+sHdk8ZNU/3ziosX/sodJq+672d1w8+6rrbx87atiM6dNGNg/Zd+Dglm0vx54rTW5cuPTuZY/079d3d/vrn/3i0utuubf071XHkkAdAyBX1LFuWU5jQR0DyLQLzj3z5fVLtn130bOP3Pni44svOv/s2BOlmFN4J6jzcNdjTz59ytgRqxbN+ZfbPhlCeOInW8v6CepYEqhjAOSKOlaQ8TQW1DGATKurq5s4fuSkU0Y31NfHniX1nMI7EQ31fX6+9ivrl83/8eZfLlzy7RDC5dMuKPeHqGNJoI4BkCvqWMjqY/jfwlP5AeC4CqfwXtq+53BX16njRqiNvTPvrgd/tvXFEELL5Am9+Pbup/JnoI6FEE6qCztWLY/ySP4DB9+84741P9iwZWTzkA/82btb33/+oIGlpt6m4WHfa+Hl57PwVP7RnsoPwPF4Kn8u0lhQxwCgBIVTeLGnSLfF82d970dP37jwX+ctWnH5tPcO6N+33J/QtzGEEKZOrfxsUWzaFOe6Ny5cuvqxDe86+/TCo9++96PNX58/q/Rv765jjYOqN2ONnFQX3tzz4rNzPz3lZnUMgLeX8zqWlzQW1DEAoGp27tk7/ZMLz3nHqXd9fuZVrRctvGd1+959z72w/bwpp8UeLY+KH/32avvrF1xxQ7mPfgshNA0PO7aGoUOqMWCt9RsSdrzwYuwpAEi07jo2ZOyUql7opD5hwLBx//b/hr/8alLqWI7SWFDHAIDqGDNi6IGDh9as21hf3+fkwQPb9+4b2Nh/8sSxsefKqcKj30IIG57+xfKHnwi9evRbwZgxlRwsoh07Yk8AQOKNOueSzf86a8TEd1b9QhP/6PmdZf+tVfXkK40FdQwAqI57bpn9P//3Xasf2xBCaB7atOjzHxs4oF/sofLuBB/9BgA5NOlPPlyDqzz/oxU1uEqJcpfGgjoGAFTBu84+fdOqha/saj/c1TV+9PCG+ux/Dnjynfij3wCAzMvpi7bWxZ2H927b8vCXYw8CAGRHXV3dhDHDJ44bqYvFtXPP3guv/Nzszy8ePfzkq1ovah7adKDj0HMvbI89FwCQRHk8NVbg7BgAQCZ59BsAULpc/5Wms2MAAJl0zy2zm4c2rX5sw33fWtc8tOneWz/h0W8AwNvK76mxAmfHAACyx6PfAIAS5T2NBXUMACCLCo9+iz0F8W26+sOdR8KR2GOkV8vnbm46qyX2FABVJI2FoI4BAEBGHeoK/fuE8ZNC48DYo6TQ/v1hy/y56hiQbdLY76hjAACQPV1HwvBTwivPq2O9MWhQOHVSpurYpqs/3BXCYccIe+vU/z59zOXTY08BFSaN/Z46BgAA2dO3UR3rvYzVscIpwhGjQ/Oo2KOk0P794aVvLQ8hqGNkjCeS/gGfWclx/WbHL2KPkDjWBKDXXt0dewLIh+461nEg9igp1F3H9m3dEnuWE1U4Rdj+amh/NfYoKVS4E1761vKdq5bHngUqSRp7K3WMY+jfELY98iUlqJg1SaY+fcKLmx6KPQVwHOf9w8379qhjUCPq2InIUh0r3AnqWO+oY2SSNPY21DGOpnVJpxL0FtYkma64t/O1Zx5SxyDhmqa0qGNJcMaHp7+8ozH2FNSCOnYi1DEK1DGyRxp7e+oYR6ME9WRNkkkdqyy3N1WijiXBmMunT/hvl6pj6bW/49CNC5dO/dCnLr1m3j0PPvbG/oPH+GJ17ESoYxSoY2SMx/AflafyczStSzrbZjRse+RLZ/7FdSePnRx7nESwJsl0xb2dKz/6UAhh4tRLYs+SboX46/amSgp17Kfz5oYQRo2MPU1eFR4p/fK/r5kwtiP2LJRt1ty7H1iz/oJzz9y5Z+/Hb/raw49vWnnnnGN8fXcdo3dOqgvbbpk79f4VsQc5UYU74bVfhRA8lb9s3XUseCo/6SeNHYs6xtEoQT1Zk2RSxyrC7U21qWNJoI6lVOfhww+t23jahFE/WDZ/5+69E9838/tPPXPc7+rbGEIIU6dWfbys2rQp9gRHt7/j0PxFDz76w5+OGTH08ovf+9cf+NPBgwYc7YvVsROhjpEZ0thxqGMcjbfKPVmTZFLHKsLtTbUV1zFiUcfSqKG+vn3j0hDCDzc9+y8rvxdCuLL1wthDEVPvThG+9iu/WdlLJ9WFHauWS2OkmjR2fOoYR+Otck/WJJnUsYpwe1Nt3XWssT72KDmmjqXXDQvu27j5lyGE86acFnsWonGKMIoknyKEUkhjJVHHOBpvlXuyJsmkjlWE25tqK9SxbbfO9TYjou46Vh/UsTRZ8dUbHn5806y5d8+57f6rPnhRY/9+sSciAqcIgV6QxkqljnE03ir3ZE2SSR2rCLc31dY0pSUDD7dOu0Id27Fq+c6dsUfheLbvav/zq2+a+keTvnn79TOmT/v8Vx7Y3f76lm0vn3/OGbFHIyanCMmz/n3D06sXnHPZsX6VmGLSWBnUMY7GW+WerEkyqWMV4faGPBhz+fRBZ7Xs37ol9iCVMTb2ANUzbnTzgY5DK9Y+2VBfP2zIoN3trzcNHDDljPGx5yIypwjJs9bFnW0zX1LHSieNlUcd42i8Ve7JmiSTOlYRbm/Ig6YpLU1TWmJPwfGtvGvOFbMXPLBmfQhhZPOQb37pU4Ma+8ceijicIoQCdawsfWIPkD6tizsP79225eEvxx6ExGld0tm/IWx75Eu/2fGL2LMkhTVJpivu7XztmYde3PRQ7EHSze0NkBAXnHvmy+uXbPvuomcfufPFxxdfdP7ZsScimu5ThFd/5o5r593jFCF51rq4M+x/6enVC2IPkgLSWG+kt46lceZ08Va5J2uSTOpYRbi9M8yeJoFdoHR1dXUTx4+cdMrohnqf8Jp3K++aM7J5yANr1i/6xtqRzUNW/fONThGSW+pYiaSxXkppHUvjzKnjrXJP1iSZ1LGKcHtnkj1NArsA9I5ThFBMHSuFNNZ7aaxjaZw5jbxV7smaJJM6VhFu7+yxp0lgF4Bec4oQiqljxyWNnZA0lqY0zpxGXtD3ZE2SSR2rCLd39tjTJLALAFAR6tixSWMnKo2lKY0zp5EX9D1Zk2RSxyrC7Z099jQJ7EIG9O8bdvzs32JPAZB36tgxSGMVkMbSlMaZ08gL+p6sSTKpYxXh9s4ee5oEdiHtWhd3vvbMd+LWsf0HIl4cICnUsaORxiojjaUpjTOnkRf0PVmTZFLHKsLtnT3p2tP9HYduXLh06oc+dek18+558LE39h+MPVFlpGsXCrK6F70Tt46d9w8373pRHUuK/n3DlrV3xJ4C8ksde1vSWMWksTSlceY0SuML+mqzJsmkjlWE2zt7UrSns+be/U/3rG7s32/nnr0fv+lrf//Zr8SeqGJStAsFGd6L3olYx5qmtKhjydG6uPPwr5+LW8de3R3x4hCfOtaTNFZJaSxNaZw5jVL3gr4GrEkyqWMV4fbOnlTsaefhww+t23jahFE/WDa/7e7PhRC+/9QzsYeqpFTsQkHm96J31DEK4tax8/7h5n171DHyTh17C2mswtJYmtI4cxql6AV9zViTZFLHKsLtnT3J39OG+vr2jUufe/SuJ36y9aY7loUQrmy9MPZQFZb8XSjIw170jjpGQcQ6VrgT1DFQx4pJY5WXxtKUxpnTKC0v6GvJmiSTOlYRbu/sScue3rDgvvtXrQshnDfltNizVF5adqEg23vRO+oYBeoYRKeOdZPGqiKNpSmNM6dRul7Q14Y1SSZ1rCLc3tmTij1d8dUbFt38sRDCnNvu7zj0ZuxxKi8Vu1CQ+b3oHXWMAnUMolPHCqSxakljaUrjzGmUohf0NWNNkkkdqwi3d/Ykdk+372p/x8Wzr7r+9rGjhs2YPm1k85B9Bw5u2fZy7LmqIrG7UJCrveidJNQxkkAdg+jUsRBCQ+wBsqx1cWfbzG1bHv5yy19dG3uWUqVx5jRqXdLZNqNh2yNfOvMvrjt57OTY4ySCNUmmK+7tXPnRh0IIE6deEnuWFHN7Z08y93Tc6OYDHYdWrH2yob5+2JBBu9tfbxo4YMoZ42PPVS3J3IWCvO1F77Qu7myb+Z0Qwthz/7LGly40kW23zt20qcZX5m20Lu5sm/nclrV3tHzgf9X40oU74afz5tb4upA0rYs722a+9PTqBedcNif2LHFIY9WVxtKUxpnTKMkv6GOxJsmkjlWE2zt7krmnK++ac8XsBQ+sWR9CGNk85Jtf+tSgxv6xh6qiZO5CQd72onfi1rGp96+o8UU5miTUscb6Gl+ZRHjqGzfGHiEpuutYXexJopDGqi6NpSmNM6dRkl/Qx2JNkkkdqwi3d/YkcE8vOPfMl9cveWn7nsNdXaeOG9FQn/23egnchYIc7kXvRKxjJEr0OuYUYQ4dORJe3/VC7CkSpFDHQgjP/yh3f3MgjdVCGktTGmdOo8S+oI/ImiSTOlYRbu/sSeCe1tXVTRw/MvYUNZXAXSjI4V70Tnrr2P6OQ/MXPfjoD386ZsTQyy9+719/4E8HDxoQe6gUi1vHnCLModYlnbFHSJzWxZ1tMxp++VRN/u+QpPNp0liNpLE0pXHmNErsC/qIrEkyqWMV4fbOHnuaBHYh7VJax2bNvfuBNesvOPfMnXv2fvymrz38+KaVd+b0MT2VErGOAQX5LIY+obJ20vj5j2mcOY0S/jFbUViTZPKZlRXh9s4ee5oEdiHtIn5mZe90Hj780LqNp00Y9YNl89vu/lwI4ftPPRN7qCyI+JmVJ2h/x6EbFy6d+qFPXXrNvHsefOyN/QdjTwSUShqrqTSWpjTOnEZe0PdkTZJJHasIt3f22NMksAtpl6461lBf375x6XOP3vXET7bedMeyEMKVrRfGHiojUlrHZs29+5/uWd3Yv1/hFOHff/YrsScCSiWN1VoaS1MaZ04jL+h7sibJpI5VhNs7e+xpEtiFtEtXHSu4YcF9969aF0I4b8ppsWfJjtTVMacIIdWksQjSWJrSOHMaeUHfkzVJJnWsItze2WNPk8AupF3q6tiKr96w6OaPhRDm3HZ/x6E3Y4+THemqY04RQqpJY3GksTSlceY08oK+J2uSTOpYRbi9s8eeJoFdSLtU1LHtu9rfcfHsq66/feyoYTOmTxvZPGTfgYNbtr0ce65MSVcdK3CKENJIGosmjaUpjTOnkRf0PVmTZFLHKsLtnT32NAnsQtolv46NG918oOPQirVPXv2ZO66dd8/u9tebBg6Ycsb42HNlTerqmFOEkEbSWExpLE1pnDmNvKDvyZokkzpWEW7v7LGnSWAX0i75dWzlXXNGNg95YM36Rd9YO7J5yKp/vnFQY//YQ2VQKuqYU4SQatJYZGksTWmcOY28oO/JmiSTOlYRbu/ssadJYBfSLuF17IJzz3x5/ZJt31307CN3vvj44ovOPzv2RJmV/DrmFCGkmjQWXxpLUxpnTqPuF/SxB0kQa5JM6lhFeA+fPfY0CexC2iW8jtXV1U0cP3LSKaMb6utjz5Jxya9jThFCejXEHoAQQmhd3Nk2c9uWh7/c8lfXxp6lVGmcOY1al3S2zWgIITx6+0diz5IU1iSZrri3c+VHH+pXH/a+8vPYs6RY4fbe9siXzvyL604eOzn2OFSAPU0Cu5B2rYs722Z+J/YUxNe6uLNt5nNb1t7R8oH/FXuWt1E4RfjS9j2Hu7pOHTdCLYUUkcaSors0xR6kDGmcOY1al3TGHiFxrEkyXXFv55prGn68/AuxB0m34vfwsWehMuxpEtiFtCvUsboQnv/RitizEFPC61jhFGHsKYCySWMJUihNdSFsTM8byzTODFTPpV9XLSug+z38gJNij0KF2NMksAtp17q4s21Gwy+fksZSoq5aP7i7jlXrAkD+VO3fWPRW4TfFqqRKZ22qOvOxOT0EZFXbjIY6/5X+Q2lvr/a0p9rvqV3oKe3/zyKf2mY21B0Jk/7kw7EHoSTP/2iFf9WQZF4aAAAAkDJtMxp8qlyKtC6WxgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAy6/8HnR4+wqxH+5gAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_model_name(given_image=None):\n",
        "  if given_image is None:\n",
        "    return \"gemini-pro\"\n",
        "  else:\n",
        "    return \"gemini-pro-vision\"\n",
        "\n",
        "def construct_image_part(given_image):\n",
        "  return {\n",
        "    \"mime_type\": \"image/jpeg\",\n",
        "    \"data\": given_image\n",
        "  }\n",
        "\n",
        "def construct_blog_part(given_blob, mime_type=\"pdf/application\"):\n",
        "  return {\n",
        "      \"mime_type\": mime_type,\n",
        "      \"data\": given_blob\n",
        "  }\n",
        "\n",
        "def call_gemini(prompt=\"\", API_KEY=None, given_text=None, given_image=None, given_blob=None, generation_config=None, safety_settings=None):\n",
        "  import google.generativeai as genai\n",
        "  genai.configure(api_key=API_KEY)\n",
        "\n",
        "  if generation_config is None:\n",
        "    generation_config = {\n",
        "      \"temperature\": 0.4,\n",
        "      \"top_p\": 1,\n",
        "      \"top_k\": 32,\n",
        "      \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "  if safety_settings is None:\n",
        "    safety_settings = [\n",
        "      {\n",
        "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "        \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "      },\n",
        "      {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "      },\n",
        "      {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "      },\n",
        "      {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "      },\n",
        "    ]\n",
        "\n",
        "  model_name = determine_model_name(given_image)\n",
        "  model = genai.GenerativeModel(model_name=model_name,\n",
        "                                generation_config=generation_config,\n",
        "                                safety_settings=safety_settings)\n",
        "\n",
        "  USER_PROMPT = prompt\n",
        "  if given_text is not None:\n",
        "    USER_PROMPT += f\"\"\"{prompt}\n",
        "  ------------------------------------------------\n",
        "  {given_text}\n",
        "  \"\"\"\n",
        "  prompt_parts = [USER_PROMPT]\n",
        "  if given_image is not None:\n",
        "    prompt_parts.append(construct_image_part(given_image))\n",
        "  if given_blob is not None:\n",
        "    prompt_parts.append(construct_blog_part(given_blob))\n",
        "\n",
        "  response = model.generate_content(prompt_parts)\n",
        "  return response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aZlduBW44Uil",
        "outputId": "0a262479-cac3-4c5a-bf3c-fb7541bb6718"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def find_json_snippet(raw_snippet):\n",
        "\tjson_parsed_string = None\n",
        "\n",
        "\tjson_start_index = raw_snippet.find('{')\n",
        "\tjson_end_index = raw_snippet.rfind('}')\n",
        "\n",
        "\tif json_start_index >= 0 and json_end_index >= 0:\n",
        "\t\tjson_snippet = raw_snippet[json_start_index:json_end_index+1]\n",
        "\t\ttry:\n",
        "\t\t\tjson_parsed_string = json.loads(json_snippet, strict=False)\n",
        "\t\texcept:\n",
        "\t\t\traise ValueError('failed to parse string into JSON format')\n",
        "\telse:\n",
        "\t\traise ValueError('No JSON code snippet found in string.')\n",
        "\n",
        "\treturn json_parsed_string\n",
        "\n",
        "def parse_first_json_snippet(snippet):\n",
        "\tjson_parsed_string = None\n",
        "\n",
        "\tif isinstance(snippet, list):\n",
        "\t\tfor snippet_piece in snippet:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tjson_parsed_string = find_json_snippet(snippet_piece)\n",
        "\t\t\t\treturn json_parsed_string\n",
        "\t\t\texcept:\n",
        "\t\t\t\tpass\n",
        "\telse:\n",
        "\t\ttry:\n",
        "\t\t\tjson_parsed_string = find_json_snippet(snippet)\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(e)\n",
        "\t\t\traise ValueError()\n",
        "\n",
        "\treturn json_parsed_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "x3jCu0ZoYv2k",
        "outputId": "f510b7fd-f184-4a86-a323-6d95c061fb3d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =' '.join(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mh0ohRYSYJBb",
        "outputId": "1e250eb2-a7b8-4480-8723-6690e756a99b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "come up with the 10 questions and answers that could be commonly asked by people about the following paper.\n",
        "There should be two types of answers included, one for expert and the other for ELI5.\n",
        "Your response should be recorded in a JSON format as ```json{\"title\": text, \"summary\": text, \"qna\": [{\"question\": \"answers\": {\"eli5\": text, \"expert\": text}}, ...]}```\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0EcCv4mIWVQz",
        "outputId": "26d9a671-5186-4944-8fd3-b047b502f104"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qna_json = None\n",
        "cur_retry = 0\n",
        "retry_num = 5\n",
        "\n",
        "while qna_json is None and cur_retry < retry_num:\n",
        "  try:\n",
        "    qna = call_gemini(\n",
        "        prompt=prompt,\n",
        "        given_text=text,\n",
        "        API_KEY=GEMINI_API_KEY\n",
        "    )\n",
        "\n",
        "    qna_json = parse_first_json_snippet(qna)\n",
        "  except:\n",
        "    cur_retry = cur_retry + 1\n",
        "    print(\"retry\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "U11PgP2yyFcD",
        "outputId": "b329cd04-b779-4a0c-e48c-8df1b4c12394"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qna_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7k4KGMGX9Zps",
        "outputId": "ff1aa14c-4976-4b25-d98d-a981fdadfa34"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers',\n",
              " 'summary': 'This paper proposes a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources. The proposed network is composed of quantized convolutional layers with 3x3 kernels and a fully connected layer at the output. It is designed for having a low memory footprint of less than 0.5MB. The proposed network is trained and evaluated on the WiderFace dataset [18]. Furthermore, to showcase multi-object detection capability, while keeping the network small, it has been trained and evaluated on a sub-set of the PascalVOC [19] dataset (3 out of the 20 classes, namely: person, chair and car). Finally, the network is deployed quantized and memory-efficient on different microcontrollers, such as the STM32H7A3 and STM32L4R9 from STMicroelectronics, Ambiq’s Apollo4b and on a novel microcontroller, MAX78000 from Analog Devices, which has a built-in CNN accelerator. The performance of the different architectures is compared and it will be shown, how the MAX78000 outperforms the other microcontrollers. Furthermore, this paper investigates the effect of mAP against relative object size dimensions within images. This evaluation helps to understand which object size should be chosen when training the generalized object detection network.',\n",
              " 'qna': [{'question': 'What is the main contribution of this paper?',\n",
              "   'answers': {'eli5': 'The paper presents a new object detection network called TinyissimoYOLO, which is designed to be small and efficient enough to run on low-power microcontrollers. This network can be used to detect objects in real-time on devices like smartphones and embedded systems.',\n",
              "    'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}},\n",
              "  {'question': 'What are the key features of TinyissimoYOLO?',\n",
              "   'answers': {'eli5': 'TinyissimoYOLO is a small and efficient object detection network that can run on low-power microcontrollers. It is designed to have a low memory footprint of less than 0.5MB and can be deployed on a variety of different microcontrollers.',\n",
              "    'expert': 'The key features of TinyissimoYOLO are: \\n- It is a quantized network, which means that the weights of the network are stored in a low-precision format, reducing the memory footprint of the network.\\n- It is a highly accurate network, achieving a mean average precision (mAP) of up to 45% on the WiderFace dataset.\\n- It is an efficient network, achieving an inference time of 5.5 ms on the MAX78000 microcontroller.'}},\n",
              "  {'question': 'What are the potential applications of TinyissimoYOLO?',\n",
              "   'answers': {'eli5': 'TinyissimoYOLO can be used in a variety of applications where real-time object detection is required on low-power devices. Some potential applications include: \\n- Security and surveillance \\n- Industrial automation \\n- Healthcare \\n- Robotics',\n",
              "    'expert': 'The potential applications of TinyissimoYOLO include: \\n- Security and surveillance: TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity.\\n- Industrial automation: TinyissimoYOLO can be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt.\\n- Healthcare: TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions.\\n- Robotics: TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.'}},\n",
              "  {'question': 'What are the limitations of TinyissimoYOLO?',\n",
              "   'answers': {'eli5': 'TinyissimoYOLO is a small and efficient network, but it does have some limitations. One limitation is that it can only detect a limited number of objects at a time. Another limitation is that it is not as accurate as larger object detection networks.',\n",
              "    'expert': \"The limitations of TinyissimoYOLO include: \\n- It can only detect a limited number of objects at a time. This is because the network has a small number of convolutional layers, which limits the network's ability to learn complex features.\\n- It is not as accurate as larger object detection networks. This is because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\"}},\n",
              "  {'question': 'What are the future directions for research on TinyissimoYOLO?',\n",
              "   'answers': {'eli5': 'There are several directions for future research on TinyissimoYOLO. One direction is to explore different network architectures to improve the accuracy of the network. Another direction is to explore different quantization techniques to reduce the memory footprint of the network. Finally, another direction is to explore different deployment platforms for the network, such as FPGAs and ASICs.',\n",
              "    'expert': 'The future directions for research on TinyissimoYOLO include: \\n- Exploring different network architectures to improve the accuracy of the network. This could involve using deeper networks, wider networks, or different types of convolutional layers.\\n- Exploring different quantization techniques to reduce the memory footprint of the network. This could involve using different quantization algorithms or different quantization formats.\\n- Exploring different deployment platforms for the network, such as FPGAs and ASICs. This could involve developing custom hardware accelerators for the network or porting the network to different types of hardware.'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qna_json['summary']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "EGvnxGpLYqEc",
        "outputId": "5d360ed3-3078-4bc3-9b19-c1a029d0033d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This paper proposes a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources. The proposed network is composed of quantized convolutional layers with 3x3 kernels and a fully connected layer at the output. It is designed for having a low memory footprint of less than 0.5MB. The proposed network is trained and evaluated on the WiderFace dataset [18]. Furthermore, to showcase multi-object detection capability, while keeping the network small, it has been trained and evaluated on a sub-set of the PascalVOC [19] dataset (3 out of the 20 classes, namely: person, chair and car). Finally, the network is deployed quantized and memory-efficient on different microcontrollers, such as the STM32H7A3 and STM32L4R9 from STMicroelectronics, Ambiq’s Apollo4b and on a novel microcontroller, MAX78000 from Analog Devices, which has a built-in CNN accelerator. The performance of the different architectures is compared and it will be shown, how the MAX78000 outperforms the other microcontrollers. Furthermore, this paper investigates the effect of mAP against relative object size dimensions within images. This evaluation helps to understand which object size should be chosen when training the generalized object detection network.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qna_json['qna']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "ddcUjLlyfZqx",
        "outputId": "695e7dc2-833f-4ab0-8e13-869f1d7650c0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'What is the main contribution of this paper?',\n",
              "  'answers': {'eli5': 'The paper presents a new object detection network called TinyissimoYOLO, which is designed to be small and efficient enough to run on low-power microcontrollers. This network can be used to detect objects in real-time on devices like smartphones and embedded systems.',\n",
              "   'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}},\n",
              " {'question': 'What are the key features of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO is a small and efficient object detection network that can run on low-power microcontrollers. It is designed to have a low memory footprint of less than 0.5MB and can be deployed on a variety of different microcontrollers.',\n",
              "   'expert': 'The key features of TinyissimoYOLO are: \\n- It is a quantized network, which means that the weights of the network are stored in a low-precision format, reducing the memory footprint of the network.\\n- It is a highly accurate network, achieving a mean average precision (mAP) of up to 45% on the WiderFace dataset.\\n- It is an efficient network, achieving an inference time of 5.5 ms on the MAX78000 microcontroller.'}},\n",
              " {'question': 'What are the potential applications of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO can be used in a variety of applications where real-time object detection is required on low-power devices. Some potential applications include: \\n- Security and surveillance \\n- Industrial automation \\n- Healthcare \\n- Robotics',\n",
              "   'expert': 'The potential applications of TinyissimoYOLO include: \\n- Security and surveillance: TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity.\\n- Industrial automation: TinyissimoYOLO can be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt.\\n- Healthcare: TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions.\\n- Robotics: TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.'}},\n",
              " {'question': 'What are the limitations of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO is a small and efficient network, but it does have some limitations. One limitation is that it can only detect a limited number of objects at a time. Another limitation is that it is not as accurate as larger object detection networks.',\n",
              "   'expert': \"The limitations of TinyissimoYOLO include: \\n- It can only detect a limited number of objects at a time. This is because the network has a small number of convolutional layers, which limits the network's ability to learn complex features.\\n- It is not as accurate as larger object detection networks. This is because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\"}},\n",
              " {'question': 'What are the future directions for research on TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'There are several directions for future research on TinyissimoYOLO. One direction is to explore different network architectures to improve the accuracy of the network. Another direction is to explore different quantization techniques to reduce the memory footprint of the network. Finally, another direction is to explore different deployment platforms for the network, such as FPGAs and ASICs.',\n",
              "   'expert': 'The future directions for research on TinyissimoYOLO include: \\n- Exploring different network architectures to improve the accuracy of the network. This could involve using deeper networks, wider networks, or different types of convolutional layers.\\n- Exploring different quantization techniques to reduce the memory footprint of the network. This could involve using different quantization algorithms or different quantization formats.\\n- Exploring different deployment platforms for the network, such as FPGAs and ASICs. This could involve developing custom hardware accelerators for the network or porting the network to different types of hardware.'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig0_desc = call_gemini(\n",
        "    prompt=f'Below is the summary of the academic paper, {qna_json[\"title\"]}. '\n",
        "           'Based on the summary, give me the description of the given figure. '\n",
        "           'Give me your response by filling in the following JSON. '\n",
        "           '{\"description\": text}',\n",
        "    given_text=qna_json['summary'],\n",
        "    given_image=figures[0],\n",
        "    API_KEY=GEMINI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "NBo9-DyP-xgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig0_desc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OqiOW3CcX7hT",
        "outputId": "774c1126-bba7-4949-9330-88d20bf95209"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' {\"description\": \"The figure shows the architecture of the proposed TinyissimoYOLO network. It consists of a series of convolutional layers, each followed by a batch normalization layer and a ReLU activation function. The first convolutional layer has a kernel size of 3x3 and a stride of 2, and the remaining convolutional layers have a kernel size of 3x3 and a stride of 1. The network also includes two max pooling layers, each with a kernel size of 2x2 and a stride of 2. The final layer is a fully connected layer that outputs the class probabilities and bounding boxes for the detected objects.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze deeper"
      ],
      "metadata": {
        "id": "hjSkq7AzwebO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Title"
      ],
      "metadata": {
        "id": "CHzelDmawqy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f\"# {qna_json['title']}\")"
      ],
      "metadata": {
        "id": "L08gV3DMbIoT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "e32b2b51-6628-4111-efc8-7067c0e53e48"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers"
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "6J1k0N_iwyXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f\"### {qna_json['summary']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "zw-i8qViwouk",
        "outputId": "5069cbcf-ec13-4b86-96de-65fd67a2ae94"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### This paper proposes a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources. The proposed network is composed of quantized convolutional layers with 3x3 kernels and a fully connected layer at the output. It is designed for having a low memory footprint of less than 0.5MB. The proposed network is trained and evaluated on the WiderFace dataset [18]. Furthermore, to showcase multi-object detection capability, while keeping the network small, it has been trained and evaluated on a sub-set of the PascalVOC [19] dataset (3 out of the 20 classes, namely: person, chair and car). Finally, the network is deployed quantized and memory-efficient on different microcontrollers, such as the STM32H7A3 and STM32L4R9 from STMicroelectronics, Ambiq’s Apollo4b and on a novel microcontroller, MAX78000 from Analog Devices, which has a built-in CNN accelerator. The performance of the different architectures is compared and it will be shown, how the MAX78000 outperforms the other microcontrollers. Furthermore, this paper investigates the effect of mAP against relative object size dimensions within images. This evaluation helps to understand which object size should be chosen when training the generalized object detection network."
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnAs"
      ],
      "metadata": {
        "id": "UdhOGoxFxp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for qna in qna_json['qna']:\n",
        "  q = qna['question']\n",
        "  a_eli5 = qna['answers']['eli5']\n",
        "  a_expert = qna['answers']['expert']\n",
        "\n",
        "  print(f\"⦿ {q}\")\n",
        "  print(f\"-- ELI5: {a_eli5}\")\n",
        "  print(f\"-- Expert: {a_expert}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x4dS4u4tw1H5",
        "outputId": "cf52d733-672f-4d64-fd50-593e06534cdd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⦿ What is the main contribution of this paper?\n",
            "-- ELI5: The paper presents a new object detection network called TinyissimoYOLO, which is designed to be small and efficient enough to run on low-power microcontrollers. This network can be used to detect objects in real-time on devices like smartphones and embedded systems.\n",
            "-- Expert: The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.\n",
            "\n",
            "⦿ What are the key features of TinyissimoYOLO?\n",
            "-- ELI5: TinyissimoYOLO is a small and efficient object detection network that can run on low-power microcontrollers. It is designed to have a low memory footprint of less than 0.5MB and can be deployed on a variety of different microcontrollers.\n",
            "-- Expert: The key features of TinyissimoYOLO are: \n",
            "- It is a quantized network, which means that the weights of the network are stored in a low-precision format, reducing the memory footprint of the network.\n",
            "- It is a highly accurate network, achieving a mean average precision (mAP) of up to 45% on the WiderFace dataset.\n",
            "- It is an efficient network, achieving an inference time of 5.5 ms on the MAX78000 microcontroller.\n",
            "\n",
            "⦿ What are the potential applications of TinyissimoYOLO?\n",
            "-- ELI5: TinyissimoYOLO can be used in a variety of applications where real-time object detection is required on low-power devices. Some potential applications include: \n",
            "- Security and surveillance \n",
            "- Industrial automation \n",
            "- Healthcare \n",
            "- Robotics\n",
            "-- Expert: The potential applications of TinyissimoYOLO include: \n",
            "- Security and surveillance: TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity.\n",
            "- Industrial automation: TinyissimoYOLO can be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt.\n",
            "- Healthcare: TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions.\n",
            "- Robotics: TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.\n",
            "\n",
            "⦿ What are the limitations of TinyissimoYOLO?\n",
            "-- ELI5: TinyissimoYOLO is a small and efficient network, but it does have some limitations. One limitation is that it can only detect a limited number of objects at a time. Another limitation is that it is not as accurate as larger object detection networks.\n",
            "-- Expert: The limitations of TinyissimoYOLO include: \n",
            "- It can only detect a limited number of objects at a time. This is because the network has a small number of convolutional layers, which limits the network's ability to learn complex features.\n",
            "- It is not as accurate as larger object detection networks. This is because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\n",
            "\n",
            "⦿ What are the future directions for research on TinyissimoYOLO?\n",
            "-- ELI5: There are several directions for future research on TinyissimoYOLO. One direction is to explore different network architectures to improve the accuracy of the network. Another direction is to explore different quantization techniques to reduce the memory footprint of the network. Finally, another direction is to explore different deployment platforms for the network, such as FPGAs and ASICs.\n",
            "-- Expert: The future directions for research on TinyissimoYOLO include: \n",
            "- Exploring different network architectures to improve the accuracy of the network. This could involve using deeper networks, wider networks, or different types of convolutional layers.\n",
            "- Exploring different quantization techniques to reduce the memory footprint of the network. This could involve using different quantization algorithms or different quantization formats.\n",
            "- Exploring different deployment platforms for the network, such as FPGAs and ASICs. This could involve developing custom hardware accelerators for the network or porting the network to different types of hardware.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More QnAs"
      ],
      "metadata": {
        "id": "M-yV7usDxsC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_prompt = \"\"\"\n",
        "Paper title: %s\n",
        "Previous question: %s\n",
        "The answer on the previous question: %s\n",
        "\n",
        "Based on the previous question and answer above, and based on the paper content below,\n",
        "Then, suggest follow-up question and answers in %s manner.\n",
        "\n",
        "There should be two types of answers included, one for expert and the other for ELI5.\n",
        "Your response should be recorded in a JSON format as ```json{\"follow up question\": text, \"answer\": {\"eli5\": text, \"expert\": text}}```\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dh6otXWVyMcc",
        "outputId": "3ecd8088-4bb1-4264-a676-67da2d2952cd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title = qna_json['title']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Inwi7rUnyxrn",
        "outputId": "a192cb2e-7d58-4f90-9b69-ea055d9df2ae"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_out(prompt, given_text, given_image=None, gemini_api_key=GEMINI_API_KEY, retry_num=3):\n",
        "  qna_json = None\n",
        "  cur_retry = 0\n",
        "\n",
        "  while qna_json is None and cur_retry < retry_num:\n",
        "    try:\n",
        "      qna = call_gemini(\n",
        "          prompt=prompt,\n",
        "          given_text=given_text,\n",
        "          given_image=given_image,\n",
        "          API_KEY=gemini_api_key\n",
        "      )\n",
        "\n",
        "      qna_json = parse_first_json_snippet(qna)\n",
        "    except:\n",
        "      cur_retry = cur_retry + 1\n",
        "      print(\"retry\")\n",
        "\n",
        "  return qna_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZvFsGf4m0o23",
        "outputId": "789d3f8e-a07a-4419-9a9c-e7046cb6c3ea"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "qnas = copy.deepcopy(qna_json['qna'])\n",
        "\n",
        "for qna in qnas:\n",
        "  q = qna['question']\n",
        "  a_expert = qna['answers']['expert']\n",
        "\n",
        "  depth_search_prompt = deep_prompt % (title, q, a_expert, \"in-depth\")\n",
        "  breath_search_prompt = deep_prompt % (title, q, a_expert, \"broad\")\n",
        "\n",
        "  depth_search_response = try_out(depth_search_prompt, text, gemini_api_key=GEMINI_API_KEY)\n",
        "  breath_search_response = try_out(breath_search_prompt, text, gemini_api_key=GEMINI_API_KEY)\n",
        "\n",
        "  if depth_search_response is not None:\n",
        "    qna['additional_depth_q'] = depth_search_response\n",
        "  if breath_search_response is not None:\n",
        "    qna['additional_breath_q'] = breath_search_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NaaMsopHxOmD",
        "outputId": "55179641-c5fe-428f-e5c1-00bad81b95d8"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No JSON code snippet found in string.\n",
            "retry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qnas[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "NNsj9rb01NvK",
        "outputId": "83804af4-9f27-44a2-9350-958f4d3f299b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the main contribution of this paper?',\n",
              " 'answers': {'eli5': 'The paper presents a new object detection network called TinyissimoYOLO, which is designed to be small and efficient enough to run on low-power microcontrollers. This network can be used to detect objects in real-time on devices like smartphones and embedded systems.',\n",
              "  'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'},\n",
              " 'additional_depth_q': {'follow up question': 'What is the main contribution of this paper?',\n",
              "  'answer': {'eli5': 'This paper introduces a new object detection network called TinyissimoYOLO. This network is designed to run on tiny microcontrollers with less than 0.5 MB of memory. TinyissimoYOLO is able to detect multiple objects in real-time on embedded microcontrollers, and it outperforms other networks in terms of latency, inference efficiency, and energy per inference.',\n",
              "   'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}},\n",
              " 'additional_breath_q': {'follow up question': 'What is the main contribution of this paper?',\n",
              "  'answer': {'eli5': 'This paper introduces a new object detection network called TinyissimoYOLO, which is designed to run on tiny devices with limited memory and computational resources. TinyissimoYOLO is a general object detection network that can be trained to detect any object class. It is also quantized to 8-bit integers, which further reduces computation cycles and memory requirements.',\n",
              "   'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qnas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fd-_3A7A3rvz",
        "outputId": "8424bb43-c42d-4f3e-d6b2-a136e59ecc0e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'What is the main contribution of this paper?',\n",
              "  'answers': {'eli5': 'The paper presents a new object detection network called TinyissimoYOLO, which is designed to be small and efficient enough to run on low-power microcontrollers. This network can be used to detect objects in real-time on devices like smartphones and embedded systems.',\n",
              "   'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'},\n",
              "  'additional_depth_q': {'follow up question': 'What is the main contribution of this paper?',\n",
              "   'answer': {'eli5': 'This paper introduces a new object detection network called TinyissimoYOLO. This network is designed to run on tiny microcontrollers with less than 0.5 MB of memory. TinyissimoYOLO is able to detect multiple objects in real-time on embedded microcontrollers, and it outperforms other networks in terms of latency, inference efficiency, and energy per inference.',\n",
              "    'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}},\n",
              "  'additional_breath_q': {'follow up question': 'What is the main contribution of this paper?',\n",
              "   'answer': {'eli5': 'This paper introduces a new object detection network called TinyissimoYOLO, which is designed to run on tiny devices with limited memory and computational resources. TinyissimoYOLO is a general object detection network that can be trained to detect any object class. It is also quantized to 8-bit integers, which further reduces computation cycles and memory requirements.',\n",
              "    'expert': 'The main contribution of this paper is the proposal of a quantized and highly accurate object detection convolutional neural network (CNN) based on the architecture of YOLO [4] suitable for edge processors with limited memory and computational resources.'}}},\n",
              " {'question': 'What are the key features of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO is a small and efficient object detection network that can run on low-power microcontrollers. It is designed to have a low memory footprint of less than 0.5MB and can be deployed on a variety of different microcontrollers.',\n",
              "   'expert': 'The key features of TinyissimoYOLO are: \\n- It is a quantized network, which means that the weights of the network are stored in a low-precision format, reducing the memory footprint of the network.\\n- It is a highly accurate network, achieving a mean average precision (mAP) of up to 45% on the WiderFace dataset.\\n- It is an efficient network, achieving an inference time of 5.5 ms on the MAX78000 microcontroller.'},\n",
              "  'additional_depth_q': {'follow up question': 'What effect does the size of objects within images have on the accuracy of TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'The accuracy of TinyissimoYOLO is affected by the size of objects within images. The network is trained on images with objects of a certain size, and it performs best when it is used to detect objects of a similar size. If the objects in the image are much smaller or larger than the objects that the network was trained on, the accuracy of the network may be lower.',\n",
              "    'expert': 'The accuracy of TinyissimoYOLO is affected by the size of objects within images. The network is trained on images with objects of a certain size, and it performs best when it is used to detect objects of a similar size. If the objects in the image are much smaller or larger than the objects that the network was trained on, the accuracy of the network may be lower. This is because the network has difficulty detecting objects that are too small or too large for its receptive field. The receptive field of a neuron is the area of the input image that it is responsible for processing. If an object is too small or too large for the receptive field of a neuron, the neuron will not be able to detect it. As a result, the network will not be able to accurately predict the location or class of the object.'}},\n",
              "  'additional_breath_q': {'follow up question': 'What is the effect of mAP against relative object size dimensions within images?',\n",
              "   'answer': {'eli5': 'The effect of mAP against relative object size dimensions within images is that the mAP increases as the relative object size increases. This is because larger objects are easier to detect than smaller objects. This is especially important for TinyissimoYOLO, which is designed to detect small objects.',\n",
              "    'expert': 'The effect of mAP against relative object size dimensions within images is that the mAP increases as the relative object size increases. This is because larger objects are easier to detect than smaller objects. This is especially important for TinyissimoYOLO, which is designed to detect small objects. The evaluation of the network’s accuracy by restricting the training data to images containing less or equal to 10 and 5 objects ensures that the objects are visible in the downscaled 88x88 pixel images.'}}},\n",
              " {'question': 'What are the potential applications of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO can be used in a variety of applications where real-time object detection is required on low-power devices. Some potential applications include: \\n- Security and surveillance \\n- Industrial automation \\n- Healthcare \\n- Robotics',\n",
              "   'expert': 'The potential applications of TinyissimoYOLO include: \\n- Security and surveillance: TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity.\\n- Industrial automation: TinyissimoYOLO can be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt.\\n- Healthcare: TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions.\\n- Robotics: TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.'},\n",
              "  'additional_depth_q': {'follow up question': \"What is the impact of the number of objects within an image on the TinyissimoYOLO network's accuracy?\",\n",
              "   'answer': {'eli5': 'The accuracy of the TinyissimoYOLO network is affected by the number of objects within an image. When the network is trained on images with a limited number of objects, it achieves higher accuracy compared to when it is trained on images with a larger number of objects. This is because the network has to learn to detect and classify a smaller number of objects, which makes the learning process easier.',\n",
              "    'expert': \"The TinyissimoYOLO network's accuracy is affected by the number of objects within an image. When the network is trained on images with a limited number of objects, it achieves higher accuracy compared to when it is trained on images with a larger number of objects. This is because the network has to learn to detect and classify a smaller number of objects, which makes the learning process easier. This effect is particularly pronounced when the network is trained on images with a small number of objects (e.g., less than 10 objects per image). In such cases, the network can achieve significantly higher accuracy compared to when it is trained on images with a larger number of objects.\"}},\n",
              "  'additional_breath_q': {'follow up question': 'What are the potential applications of TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity. It can also be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt. Additionally, TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions. Finally, TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.',\n",
              "    'expert': 'The potential applications of TinyissimoYOLO include:\\n- Security and surveillance: TinyissimoYOLO can be used to detect objects in real-time for security and surveillance applications, such as detecting intruders or suspicious activity.\\n- Industrial automation: TinyissimoYOLO can be used to detect objects in real-time for industrial automation applications, such as detecting defects on products or identifying objects on a conveyor belt.\\n- Healthcare: TinyissimoYOLO can be used to detect objects in real-time for healthcare applications, such as detecting tumors or other medical conditions.\\n- Robotics: TinyissimoYOLO can be used to detect objects in real-time for robotics applications, such as detecting obstacles or identifying objects for manipulation.'}}},\n",
              " {'question': 'What are the limitations of TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'TinyissimoYOLO is a small and efficient network, but it does have some limitations. One limitation is that it can only detect a limited number of objects at a time. Another limitation is that it is not as accurate as larger object detection networks.',\n",
              "   'expert': \"The limitations of TinyissimoYOLO include: \\n- It can only detect a limited number of objects at a time. This is because the network has a small number of convolutional layers, which limits the network's ability to learn complex features.\\n- It is not as accurate as larger object detection networks. This is because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\"},\n",
              "  'additional_depth_q': {'follow up question': 'What are the limitations of TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'TinyissimoYOLO can only detect a limited number of objects at a time and is not as accurate as larger object detection networks.',\n",
              "    'expert': \"TinyissimoYOLO can only detect a limited number of objects at a time because the network has a small number of convolutional layers, which limits the network's ability to learn complex features. It is not as accurate as larger object detection networks because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\"}},\n",
              "  'additional_breath_q': {'follow up question': 'What are the limitations of TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'TinyissimoYOLO can only detect a limited number of objects at a time and is not as accurate as larger object detection networks.',\n",
              "    'expert': \"The limitations of TinyissimoYOLO include: \\n- It can only detect a limited number of objects at a time. This is because the network has a small number of convolutional layers, which limits the network's ability to learn complex features.\\n- It is not as accurate as larger object detection networks. This is because the network has a small number of parameters, which limits the network's ability to learn the complex relationships between objects in an image.\"}}},\n",
              " {'question': 'What are the future directions for research on TinyissimoYOLO?',\n",
              "  'answers': {'eli5': 'There are several directions for future research on TinyissimoYOLO. One direction is to explore different network architectures to improve the accuracy of the network. Another direction is to explore different quantization techniques to reduce the memory footprint of the network. Finally, another direction is to explore different deployment platforms for the network, such as FPGAs and ASICs.',\n",
              "   'expert': 'The future directions for research on TinyissimoYOLO include: \\n- Exploring different network architectures to improve the accuracy of the network. This could involve using deeper networks, wider networks, or different types of convolutional layers.\\n- Exploring different quantization techniques to reduce the memory footprint of the network. This could involve using different quantization algorithms or different quantization formats.\\n- Exploring different deployment platforms for the network, such as FPGAs and ASICs. This could involve developing custom hardware accelerators for the network or porting the network to different types of hardware.'},\n",
              "  'additional_depth_q': {'follow up question': 'What are the future directions for research on TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'TinyissimoYOLO is a very small and efficient object detection network that can be used on low-power microcontrollers. Future research directions include exploring different network architectures to improve accuracy, exploring different quantization techniques to reduce memory footprint, and exploring different deployment platforms for the network, such as FPGAs and ASICs.',\n",
              "    'expert': 'Future directions for research on TinyissimoYOLO include:\\n- Exploring different network architectures to improve the accuracy of the network. This could involve using deeper networks, wider networks, or different types of convolutional layers.\\n- Exploring different quantization techniques to reduce the memory footprint of the network. This could involve using different quantization algorithms or different quantization formats.\\n- Exploring different deployment platforms for the network, such as FPGAs and ASICs. This could involve developing custom hardware accelerators for the network or porting the network to different types of hardware.'}},\n",
              "  'additional_breath_q': {'follow up question': 'What are the future directions for research on TinyissimoYOLO?',\n",
              "   'answer': {'eli5': 'TinyissimoYOLO is a tiny machine learning model that can detect objects in images. It is designed to run on small, low-power devices like microcontrollers. Future research on TinyissimoYOLO could focus on improving its accuracy, making it even smaller and more efficient, and exploring new applications for it.',\n",
              "    'expert': 'Future research directions for TinyissimoYOLO include:\\n- Exploring different network architectures to improve the accuracy of the network.\\n- Exploring different quantization techniques to reduce the memory footprint of the network.\\n- Exploring different deployment platforms for the network, such as FPGAs and ASICs.\\n- Developing custom hardware accelerators for the network to improve its performance.\\n- Porting the network to different types of hardware, such as microcontrollers with built-in CNN accelerators.'}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtS5RAFZ_KfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio UI"
      ],
      "metadata": {
        "id": "UDvzFjzc_QeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "S_-z2IMD_Ppp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "STYLE = \"\"\"\n",
        ".small-font{\n",
        "  font-size: 12pt;\n",
        "}\n",
        "\n",
        ".group {\n",
        "  padding-left: 10px;\n",
        "  padding-right: 10px;\n",
        "  padding-bottom: 10px;\n",
        "  border: 1px dashed black;\n",
        "  border-radius: 20px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=STYLE) as demo:\n",
        "  gr.Markdown(f\"# {qna_json['title']}\")\n",
        "  gr.Markdown(f\"{qna_json['summary']}\")\n",
        "\n",
        "  gr.Markdown(\"# Auto generated Questions & Answers\")\n",
        "\n",
        "  for qna in qnas:\n",
        "    with gr.Column(elem_classes=[\"group\"]):\n",
        "      gr.Markdown(f\"## {qna['question']}\")\n",
        "      gr.Markdown(qna['answers']['eli5'], elem_classes=[\"small-font\"])\n",
        "\n",
        "      with gr.Accordion(\"More technical answer\", open=False):\n",
        "        gr.Markdown(qna['answers']['expert'], elem_classes=[\"small-font\"])\n",
        "\n",
        "      with gr.Accordion(\"Additional question (depth)\", open=False):\n",
        "        gr.Markdown(f\"## {qna['additional_depth_q']['follow up question']}\")\n",
        "        gr.Markdown(qna['additional_depth_q']['answer']['eli5'], elem_classes=[\"small-font\"])\n",
        "\n",
        "        with gr.Accordion(\"More technical answer\", open=False):\n",
        "          gr.Markdown(qna['additional_depth_q']['answer']['expert'], elem_classes=[\"small-font\"])\n",
        "\n",
        "      with gr.Accordion(\"Additional question (breath)\", open=False):\n",
        "        gr.Markdown(f\"## {qna['additional_breath_q']['follow up question']}\")\n",
        "        gr.Markdown(qna['additional_breath_q']['answer']['eli5'], elem_classes=[\"small-font\"])\n",
        "\n",
        "        with gr.Accordion(\"More technical answer\", open=False):\n",
        "          gr.Markdown(qna['additional_breath_q']['answer']['expert'], elem_classes=[\"small-font\"])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "FT6MIDbT_SlD",
        "outputId": "826dd527-0972-4a21-b941-65ce7e1c7740"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4a527151a0527e7553.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4a527151a0527e7553.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4J8UI6s_aDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}